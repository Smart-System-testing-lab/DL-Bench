# DL-Bench Repository

This README provides detailed information on the purpose and functionality of each file in the DL-Bench repository, which is structured to benchmark and evaluate code generation capabilities of large language models (LLMs) in deep learning projects. Each file in this repository is responsible for specific tasks in the testing and manipulation of functions and classes.

---

## Data Structure

The `data` folder contains essential data files for the project. Each file represents a specific scenario or task in the project pipeline. The data files follow a structured format, providing relevant information for testing or inference purposes.

### Sample Data

An example file, `korniadraw_line41.txt`, contains the following structure:
- **Stage**: Specifies the project phase (e.g., Inference).
- **Task**: Represents the specific task being performed (e.g., drawing a line on an image).
- **Data Type**: Indicates the input type (e.g., Image).
- **Prompt**: Describes the instructions or function specifications.
- **Ground Truth**: Refers to the associated file or function path in the repository.
- **Repository**: Provides the project repository name.
- **Function**: States the function name related to the task.
- **Test Cases**: Points to the associated test cases.

These data files are integral to the project's functionality and testing. You can find all relevant files in the `data` folder.




## LLMs


### Installation

To run the script, you need the following dependencies installed:

1. **Python 3**: Ensure you have Python 3 installed on your system.
2. **Required Python Packages**:
   - `boto3`
   - `botocore`
   - `openai`
   - `google.generativeai`
   - `tqdm`

You can install the dependencies using the following pip command:
```bash
pip install boto3 botocore openai google.generativeai tqdm
```

3. **API Keys**:
   - Set up your API keys for OpenAI and Google Generative AI (Gemini) in your environment variables as `API_KEY` and `Geminay_KEY`.

## Usage

The script provides several functions to interact with various large language models (LLMs). It supports models like `OpenAI`, `Claude`, `Gemini`, `Mistral`, and `LLaMA`.

#### Command to Run
Use the following command to execute the script:
```bash
python3 call.py --model <modelname>
```

#### Supported Models
- `geminai`: Google Generative AI (Gemini)
- `gpt-4o`: OpenAI's GPT-4 optimized model
- `mistral`: Mistral 7B Instruct model
- `llama`: Meta's LLaMA model
- `antropic`: Claude model

Replace `<modelname>` with one of the above options.

#### File Structure
The script processes `.txt` files in a folder and converts their content into JSON format. The JSON content is sent as a prompt to the specified LLM, and the results are saved.

### Example

#### Running with Gemini AI
```bash
python3 call.py --model geminai
```

#### Processing Text Files
1. Prepare an input folder containing `.txt` files with the required prompt structure.
2. Define the output folder for the processed files.

Example:
```bash
python3 call.py --model mistral
```

This will process all text files in the specified input folder, send prompts to the Mistral model, and save JSON outputs.

### JSON Structure for Input and Output
#### Input Example
The `.txt` file should follow this format:
```
prompt: Describe the features of a cat.
ground truth: A cat has fur, whiskers, and a tail.
function: animal_features
test_cases: Should return a description including three traits of cats.
```

#### Output Example
Processed JSON:
```json
{
    "result": "Cats have fur, whiskers, and a tail, which help them adapt to their environment.",
    "prompt": "Describe the features of a cat.",
    "function_name": "animal_features",
    "ground_truth": "A cat has fur, whiskers, and a tail.",
    "test": "Should return a description including three traits of cats."
}
```

### Folder Structure
- **Input Folder**: `/home/x/Desktop/DLEval-20240920T201632Z-001/DLEval`
- **Output Folder**: `/local/data0/moved_data/Organized_benchmark/LLMs/output_<modelname>/<repo>`

The script processes text files in the input folder and saves results to the output folder based on the model and repository.

### Error Handling
If a model invocation fails, the script will:
- Print an error message with the reason for failure.
- Exit the program if the error persists.

Ensure your AWS region and credentials are correctly set when using AWS-based models like Mistral and LLaMA.


# Python Scripts: Usage Guide

### Overview of Testing
The scripts provided are designed to test and validate functions generated by various Large Language Models (LLMs) using test cases from specific repositories. They include mechanisms for replacing functions, running tests, and processing results.

### Installation

1. **Python 3**: Ensure you have Python 3 installed on your system.
2. **Required Python Packages**:
   - `ast`
   - `json`
   - `os`
   - `shutil`
   - `subprocess`
   - `re`
   - `tqdm`

You can install missing dependencies via pip:
```bash
pip install tqdm
```

3. **Additional Requirements**:
   - A valid Python virtual environment or Conda environment.
   - Required repositories and test cases should be placed in the specified directories.

### Usage

#### Command to Run
Execute the main script using the following command:
```bash
python3 test_runner.py --model <modelname>
```

Replace `<modelname>` with the desired LLM (e.g., `geminai`, `openai`).

---

### File Descriptions

#### 1. `test_runner.py`
This is the main script responsible for:
- Backing up and restoring files during testing.
- Running tests using `pytest` in specified environments.
- Replacing functions in a Python module with generated code.
- Comparing test results and identifying test cases affected by the modified function.

##### Functions:
- `backup_file(file_path)`: Creates a backup of the given file.
- `clear_pycache()`: Clears Python cache directories.
- `run_pytest(test_file, ...)`: Runs `pytest` on the specified test file.
- `ruin_function(file_path, function_name)`: Modifies a function to a broken state.
- `restore_file(backup_path, original_path)`: Restores the original file from the backup.
- `compare_test_results(first_result, second_result)`: Compares test results to identify changes.
- `process_test_results(...)`: Saves test results to a structured output file.
- `runner(...)`: Orchestrates the entire process of backup, replace, ruin, test, and restore.

---

#### 2. `replacer.py`
This script replaces functions in a Python file with generated code.

##### Key Features:
- Uses `ast` (Abstract Syntax Tree) to parse and modify Python code.
- Renames the original function and adds a wrapper function that calls the generated function.
- Captures and inserts required imports for the generated code.

##### Functions:
- `replace_function(repo_path, function_name, temp_file, generated_code_file)`: Replaces the specified function in the repository with generated code.

---

#### 3. `parse_code.py`
This script processes the output JSON files from LLMs to extract generated code, and it invokes the `test_runner.py` script to validate them.

##### Key Features:
- Reads and parses JSON files from the output folder.
- Extracts and formats Python code from the LLM output.
- Calls the `runner` function from `test_runner.py` to replace and test the code.

##### Variables:
- `r_s`: List of repositories to process.
- `llms`: List of LLMs to use.

---

### Example Workflow

#### Step 1: Run the Script
Run the script with the desired model:
```bash
python3 test_runner.py --model geminai
```

#### Step 2: Process Output
The script will:
1. Back up the target file.
2. Replace the specified function with LLM-generated code.
3. Run `pytest` to validate the changes.
4. Compare initial and final test results.
5. Save the results in structured output files.

#### Step 3: View Results
Results are saved in:
```
/local/data0/moved_data/Organized_benchmark/results/<model>/<repository>/
```

---

### Input and Output Structure

#### Input JSON Example
Each `.json` file should have the following structure:
```json
{
    "ground_truth": "repo/path/to/module.py",
    "function_name": "target_function",
    "test": "repo/path/to/test_file.py::test_case",
    "result": "Generated Python code..."
}
```

#### Output File Structure
Output files are saved with details about:
- Test results before and after replacing the function.
- Related test cases.
- Errors encountered.

---

### Folder Structure

- **Input Folder**: `LLMs/output_<model>/<repository>/`
- **Output Folder**: `/local/data0/moved_data/Organized_benchmark/results/<model>/<repository>/`

---

### Error Handling
The script includes robust error handling for:
- Missing functions in the repository.
- Issues during `pytest` execution.
- Invalid JSON or code parsing errors.

Ensure proper setup of environments and repository paths for smooth execution.


## File Descriptions

### 1. `call.py`

This script is primarily responsible for executing tests on individual functions and comparing test results after modifications. The `runner` function is central to this process and follows these steps:
- **Backup**: Creates a backup of the target file.
- **Initial Testing**: Runs initial tests using `pytest` to establish a baseline result.
- **Modification**: Uses the `replace_function` function to temporarily modify the target function, rerunning tests to identify which cases fail with the modification.
- **Restore**: Restores the original function from backup.
- **Final Testing**: Runs the tests once more with the restored function to validate that results match the initial run.
  
This process helps identify test cases affected by function modifications, providing insight into the impact of LLM-generated code changes.

### 2. `parse_code.py`

The `parse_code.py` file processes JSON files containing LLM-generated code and metadata. This script reads LLM outputs, extracts Python code snippets, and executes them within a testing environment. Key steps include:
- **JSON Parsing**: Reads JSON output files generated by LLMs, targeting files with `.json` extensions.
- **Code Extraction**: Uses regex patterns to identify Python code blocks in LLM outputs.
- **Test Execution**: Calls the `runner` function from `call.py` to execute tests on extracted code, capturing any differences in test results compared to the original.
  
This script is integral to the benchmarking pipeline, which evaluates LLM-generated code's accuracy and effectiveness in replicating expected functionality.

### 3. `test_runner_class.py`

`test_runner_class.py` extends the test functionality to class-based methods. This script handles metadata for class names, function names, and test cases associated with specific classes. Key functions include:
- **Class-Specific Testing**: Runs tests on methods within a class and compares results from initial, modified, and restored versions of the code.
- **Comparison**: Identifies which test cases are impacted by modifications to class methods.
  
The script facilitates in-depth testing for LLM-generated code in class contexts, helping determine compatibility and functionality of methods within class structures.

### 4. `test_runner.py`

This script provides functionality similar to `test_runner_class.py`, but is designed for individual (non-class) functions. Main operations include:
- **Test Execution**: Runs initial and modified tests for standalone functions.
- **Result Comparison**: Uses a comparison function to identify changes in test results due to modifications, assisting in isolating test cases affected by LLM code changes.
  
The script plays an essential role in evaluating the accuracy of LLM-generated functions and identifying their functional compatibility within a project.

### 5. `replacer.py`

`replacer.py` is responsible for programmatically replacing functions within a codebase. Key functionalities include:
- **Function Backup**: Backs up the target file to ensure the original function can be restored.
- **Function Wrapping**: Creates a wrapper around an existing function by renaming the original and allowing the new version to import it for isolated testing.
- **AST Manipulation**: Uses Abstract Syntax Trees (ASTs) to find, rename, and modify functions, facilitating code manipulation without changing the underlying logic.
  
The `replace_function` function is particularly useful for testing generated code in isolated contexts while maintaining the original function structure in the codebase.

### 6. `replace_class.py`

Expanding on `replacer.py`, `replace_class.py` introduces functionality for replacing methods within classes. Key features include:
- **Class and Method Identification**: Locates the target class and method within a file and replaces the method with a new definition.
- **AST Operations**: Similar to `replacer.py`, it uses ASTs to perform precise code modifications without altering the surrounding code structure.
- **Backup and Restore**: Backs up the original code and restores it after testing, ensuring modifications are reversible.
  
This file is especially useful for testing new or modified class methods generated by LLMs in deep learning projects, providing flexibility to validate changes within object-oriented contexts.

### 7. `parse_code_class.py`

The `parse_code_class.py` file processes LLM-generated JSON data specifically for classes. It follows these steps:
- **Class-Level JSON Parsing**: Reads LLM-generated JSON output files, specifically targeting class-based functions and methods.
- **Code Extraction and Storage**: Extracts code snippets for methods within classes and saves them temporarily for testing.
- **Test Execution**: Initiates tests using the `runner` function, verifying the functionality of generated methods within the target class structure.
  
This script is tailored for class-based testing, enabling the assessment of LLM-generated class methods against known ground-truth implementations.

---

Each file is integral to the benchmarking and evaluation of code generation by large language models in deep learning projects. By isolating and comparing test results for modified functions and classes, the repository enables robust assessment of LLMs' ability to produce functionally accurate code in real-world settings.
